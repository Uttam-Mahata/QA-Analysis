# -*- coding: utf-8 -*-
"""SAC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YX8Q_KEXXtbvMuXjcW3tpe1D4zeYOyF
"""

!git clone https://github.com/Uttam-Mahata/Student-Assessment-Analysis-using-NLP.git

# Commented out IPython magic to ensure Python compatibility.
# %cd Student-Assessment-Analysis-using-NLP

# Commented out IPython magic to ensure Python compatibility.
# %ls

!pip install transformers

!pip install sentence_transformers

!pip install spacy

!python -m spacy download en_core_web_sm

import os
import spacy
from sentence_transformers import SentenceTransformer
from nltk.tokenize import sent_tokenize

import nltk
nltk.download('punkt')

# Load spaCy English language model
nlp = spacy.load("en_core_web_sm")

# Load Sentence Transformer model
sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def pos_tagging(sentence, question_number, student_number, sentence_number, pos_file):
    # Use spaCy for POS tagging
    doc = nlp(sentence)

    # Extract POS tags
    pos_tags = [(token.text, token.pos_) for token in doc]

    # Save POS tags to the provided file for each sentence
    pos_file.write(f"S{sentence_number}: {', '.join([f'{pos[0]}: {pos[1]}' for pos in pos_tags])}\n")

    # Print POS tags to the terminal
    print(f"Question {question_number}, Student {student_number}, Sentence {sentence_number} - POS Tags: {pos_tags}")

def named_entity_extraction(sentence, question_number, student_number, sentence_number, ner_file):
    # Use spaCy for named entity recognition
    doc = nlp(sentence)

    # Extract named entities
    named_entities = [(ent.text, ent.label_) for ent in doc.ents]

    # Save named entities to the provided file for each sentence
    ner_file.write(f"S{sentence_number}: {', '.join([f'{ne[0]}: {ne[1]}' for ne in named_entities])}\n")

    # Print named entities to the terminal
    print(f"Question {question_number}, Student {student_number}, Sentence {sentence_number} - Named Entities: {named_entities}")

def process_student_answers():
    # Loop through each question and each student's answer
    for question_number in range(1, 11):
        for student_number in range(1, 65):  # Adjusted the range to 15 students
            # Create directories to store processed answers, POS tags, and named entities
            processed_folder = f"data/processed_data/processed_answers/question{question_number}"
            os.makedirs(processed_folder, exist_ok=True)

            pos_folder = f"data/processed_data/pos_tags/question{question_number}"
            os.makedirs(pos_folder, exist_ok=True)

            ner_folder = f"data/processed_data/named_entities/question{question_number}"
            os.makedirs(ner_folder, exist_ok=True)

            # Create files to store POS tags and named entities for each student's answer
            pos_file_path = os.path.join(pos_folder, f"student{student_number}_pos_tags.txt")
            ner_file_path = os.path.join(ner_folder, f"student{student_number}_named_entities.txt")

            with open(pos_file_path, 'w', encoding='utf-8') as pos_file, \
                 open(ner_file_path, 'w', encoding='utf-8') as ner_file:
                # Read the raw answer from the file
                answer_file_path = f"data/raw_data/answers/question{question_number}/student{student_number}_answer.txt"
                with open(answer_file_path, 'r', encoding='utf-8') as answer_file:
                    raw_answer = answer_file.read()

                # Save the processed answer to a file
                processed_answer_file_path = os.path.join(processed_folder, f'student{student_number}_processed_answer.txt')
                with open(processed_answer_file_path, 'w', encoding='utf-8') as processed_answer_file:
                    # Tokenize the answer into sentences after full stops and start a new line
                    sentences = sent_tokenize(raw_answer)
                    processed_answer_file.write('\n'.join(sentences))

                    # Process each sentence separately
                    for sentence_number, sentence in enumerate(sentences, start=1):
                        # Perform POS tagging and named entity recognition, then save to files
                        pos_tagging(sentence, question_number, student_number, sentence_number, pos_file)
                        named_entity_extraction(sentence, question_number, student_number, sentence_number, ner_file)

    print('POS tagging and named entity recognition using Sentence Transformers completed successfully.')

# Call the function to process student answers
process_student_answers()

import os
import numpy as np
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer

# Load Sentence Transformer model
sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def get_sentence_vector(sentence):
    # Use Sentence Transformer for sentence embedding
    sentence_embedding = sbert_model.encode(sentence, convert_to_tensor=True).tolist()
    return sentence_embedding

def process_student_answers():
    # Loop through each question and each student's answer
    for question_number in range(1, 11):
        for student_number in range(1, 65):  # Adjusted the range to 15 students
            # Create directory to store average vectors for each question
            avg_vector_folder = f"data/processed_data/Avg_Vectors/question{question_number}"
            os.makedirs(avg_vector_folder, exist_ok=True)

            # Create file to store average vectors for each student's answer
            avg_vector_file_path = os.path.join(avg_vector_folder, f"student{student_number}_avg_vector.txt")

            with open(avg_vector_file_path, 'w', encoding='utf-8') as avg_vector_file:
                # Read the processed answer from the file
                processed_answer_file_path = f"data/processed_data/processed_answers/question{question_number}/student{student_number}_processed_answer.txt"
                with open(processed_answer_file_path, 'r', encoding='utf-8') as processed_answer_file:
                    processed_answers = processed_answer_file.read().split('\n')

                # Vectorize each sentence and convert to numeric values
                sentence_vectors = []
                for sentence_number, sentence in enumerate(processed_answers, start=1):
                    # Use Sentence Transformer for sentence vector
                    sentence_vector = get_sentence_vector(sentence)
                    sentence_vectors.append(sentence_vector)

                if sentence_vectors:
                    # Calculate the average vector for all sentences
                    avg_vector = np.mean(sentence_vectors, axis=0)

                    # Save the average vector to the provided file for each student
                    avg_vector_file.write(f"Student{student_number}_Avg_Vector: {', '.join([f'v_w{i+1}={v}' for i, v in enumerate(avg_vector)])}\n")

                    # Print the average vector to the terminal
                    print(f"Question {question_number}, Student {student_number} - Average Vector: {avg_vector}")
                else:
                    avg_vector_file.write("No sentences to calculate the average vector.\n")

    print('Average vector calculation completed successfully.')

# Call the function to process student answers and calculate average vectors
process_student_answers()

import os
import numpy as np
import pandas as pd

def load_avg_vectors(question_number, student_number):
    avg_vector_file_path = f"data/processed_data/Avg_Vectors/question{question_number}/student{student_number}_avg_vector.txt"

    try:
        with open(avg_vector_file_path, 'r', encoding='utf-8') as avg_vector_file:
            avg_vector_line = avg_vector_file.readline()
            avg_vector_str = avg_vector_line.split(": ")[1].strip()
            avg_vector = [float(val.split("=")[1]) for val in avg_vector_str.split(", ")]
            return avg_vector
    except FileNotFoundError:
        print(f"Average vector file not found for Question {question_number}, Student {student_number}.")
        return None

def create_dataset():
    data = []

    # Loop through each question and each student's answer
    for question_number in range(1, 11):
        for student_number in range(1, 65):
            # Load the existing average vector for the student
            avg_vector = load_avg_vectors(question_number, student_number)

            if avg_vector:
                # Append data to the dataset
                data.append({
                    'Question': question_number,
                    'Student': student_number,
                    **{f'v{i+1}': avg_vector[i] for i in range(len(avg_vector))}
                })

    print('Dataset creation completed successfully.')

    # Create a DataFrame from the collected data
    df = pd.DataFrame(data)

    # Save the DataFrame to a CSV file
    df.to_csv('complete_dataset.csv', index=False)
    print('Dataset saved to complete_dataset.csv')

# Call the function to create a dataset using existing average vectors
create_dataset()



# Commented out IPython magic to ensure Python compatibility.
# %ls

!pip install torch